% vim: tw=110:fo=an1lt:nosmartindent

\chapter{Problem Definition}

In this chapter, we present an overview of the problem area and a description of what exactly we are solving.
It should also serve as an introduction for readers unfamiliar with the topic, providing explanations of terms
that we are using throughout this work.


\section{Files and filesystems}

Most modern operating systems share a common idea of what constitutes a filesystem. It is a collection of {\it
files} that are referenced through a hierarchical tree-like structure of {\it directories}.  You can identify
any file or directory by its {\it path}, which is a string of entry names delimited by path separator
characters.\footnote{On UNIX-like systems, the path separator is a forward-slash, `{\tt /}'.  Windows
recognizes both forward-slash and a backslash, but backslash is used more commonly.  Nevertheless, we will be
using forward-slash as a path separator everywhere.} You follow the (absolute) path by starting at the root
directory and always descending into the directory named by the next path element, until the last element is
remaining, naming the target entry.

A {\it file} is usually taken to mean an opaque string of binary data. This was not always the case -- for
instance, some historical systems treated files as structured collections of records. Several contemporary
operating systems support the concept of ``forks'', which are separate data streams associated with a single
filesystem object. This feature is rarely used in practice, mostly because of interoperability issues and poor
end-user application support. We will be using the single-stream interpretation of the term.

Throughout this work, the word ``filesystem'' is used to denote both the hierarchy of directories and files,
and a concrete implementation or format of this hierarchy, with particular features.  In most cases, the
intended meaning should be clear from context. Where ambiguous, the terms ``filesystem tree'' and ``filesystem
type'' will be used in order to distinguish between the concepts.

We want to be able to publish one or more subtrees of the filesystem on one computer, and access this part of
the structure from another computer over a network.


\section{Protocols}

A protocol, simply put, is a communication script for two or more parties. It describes the conversation
structure, semantics and syntax of messages. Here is an overview of several issues that must be solved by
every protocol.


\subsection{Sessions}

The most basic ``protocol'' imaginable is one computer just firing data packets at another, silently hoping
that the other will reassemble and understand them. It is, of course, possible to use this sort of protocol
for transfering files, but it is obviously very impractical. All but the simplest of protocols will first
establish a session, in order to have a context for further communication. Extent of the session can vary,
from simply marking a collection of packets as belonging together, to providing ordered delivery, to
authenticating parties and encrypting transmitted data.

Basic session functionality is often provided by the transport layer, specifically the Transmission Control
Protocol (TCP). This allows the communicating parties to establish a two-way stream of octets, with
reliability, ordered delivery and flow control. Unless noted otherwise, protocols described in this work are
built on top of TCP.


\subsection{Encoding}

Protocols consist of messages, which must be encoded for over-the-wire transfer. There are two main ways to do
this: either as human-readable text, or as custom binary data. Alternately, it is possible to store the data
in a self-describing format, which abstracts away the details of the encoding.

\subsubsection{Text-based protocols}

In text-based protocols, each message is a string of words and/or numbers, nowadays usually encoded as ASCII
or UTF-8. Many such protocols also use line separator characters to delimit messages. The main advantage of
this approach is that it is readable (and writable) by humans without specialized debugging tools -- for
instance, it is possible to communicate with the other party by typing out commands, without software
implementing the protocol.  Historically, there was also the fact that two communicating systems only had to
agree on text encoding, and not about in-memory format of numbers or other data structures.

Drawbacks include overhead of text encoding, both in terms of size and processing time spent encoding and
decoding the textual representation. Another problem is that lengths of message fields are not fixed, so
parsing and framing messages is more difficult.

\subsubsection{Binary protocols}

The alternative to text-based messages is using a format similar to in-memory encoding of data. Each message
can be described by, and parsed as, a C struct.\footnote{This does not mean that you can create a message by
dumping the memory representation of a corresponding C struct directly. On modern systems, fields of a struct
will be memory-aligned and will contain some amount of padding. This can differ between systems, compilers,
and can even depend on particular compiler flags.} This results in shorter messages and simpler encoding. If
the in-memory encoding is the same as in the protocol, encoding and decoding can even be skipped completely.
Parsing and framing is trivial -- every kind of message can either have a fixed number of fixed-size fields, or
it can have a fixed-size part describing lengths of all variable-size fields.

Care must be taken when communicating between systems that differ in endianness.  The protocol either has to
prescribe the byte order, or it must provide means for the parties to negotiate.

Unlike text-based protocols, debugging is not straightforward, humans require specialized tools that convert
the binary messages to understandable text.

\subsubsection{Self-describing data}

A data format can be considered self-describing if it carries enough information to reconstruct the data
without pre-existing knowledge of its structure. A protocol designer might use this property to describe
protocol messages in an abstract notation, such as ASN.1~\cite{asn-basic}, instead of specifying details of
on-the-wire encoding. Implementers can then use a software library that converts data between an abstract
representation and a self-describing format, itself either textual or binary.

One advantage of this approach is that it simplifies both protocol specifications and implementations by
providing a sort of ``interface'' that hides the low-level details. Obviously, this comes at the cost of
control over these details. Another advantage is that it is possible to use protocol-agnostic tools to inspect
and debug the communication.

In terms of efficiency, a binary self-describing format is probably better than a text-based protocol, but
still worse than a pure binary one, both in size and processing time. This is due to the fact that the
self-describing format must carry additional information about itself, and that this information must be
processed before data is extracted.

\subsection{Roles}

We recognize two basic roles for communicating parties: {\it client} and {\it server}. Client is the party
that sends requests -- specifies what is to be done -- and server's task is to fulfil the requests and report on
results.

Most protocols are modelled as client-server, with roles of the parties fixed in advance. Typically, the
server is listening for incoming connections and clients connect at their convenience. In a peer-to-peer
model, after establishing the connection, parties can act as both clients and servers at the same time,
sending requests to each other.

In this work, we use the following convention: a {\it server} will be the computer that contains a ``master
copy'' of a filesystem, and publishes a subtree of this filesystem on the network. A {\it client} is the
computer that wants to manipulate this published tree using the protocol.


%%%

\section{Capabilities of a file transfer protocol}

\subsection{Paths and filenames}

In order to accomplish anything useful with a protocol, there must be a way to specify on which file you want
to operate. This alone poses a number of issues that must be resolved.

\subsubsection{Relative paths}

The trivial way to represent a path is by using an absolute path string, i.e., one that specifies all path
components leading to the target object. Apart from this method, operating systems have a concept of {\it
current directory} and paths relative to the current directory. Relative path can contain entries named
``\texttt{.}'' (refers to current directory) and ``\texttt{..}'' (parent directory).\footnote{For security
reasons, protocol should prescribe, and implementations enforce, that ``\texttt{.}'' and ``\texttt{..}''
components are forbidden in an absolute path.} In some cases, it can be useful to expose this functionality
through the protocol -- either allowing the client to choose the current directory, or setting it at a fixed
path (e.g., authenticated user's home directory).

\subsubsection{Validity of path string}

Different platforms have different conventions for filenames and paths. Some characters might be invalid in
file and path names, lengths of path components or the total length of path might be limited. Protocols should
either abstract away the differences (an inherently difficult problem, usually resulting in only the
lowest-common-denominator functionality), provide means to query platform limitations, or at least acknowledge
failure modes for path names and note that they are platform-dependent.

Another problem is correspondence between directory listings and results of operations. On Windows, filenames
are not case-sensitive, so creating file {\tt foo} will fail if the directory already contains file {\tt FOO}.
Mac~OS~X normalizes unicode representations of filenames, which can mean that a newly created file will appear
under a different name.

\subsubsection{Filename character encoding}

One place where a file transfer protocol cannot avoid character encoding issues is in path specifiers and
directory entry names. The problem is that different systems have different conventions for filename encoding.
A protocol could specify that filenames are transfered as ``plain data'', but this would only push the problem
down to individual implementers. A better solution is to either prescribe or negotiate a transfer encoding,
and then transcode filenames in client and server software.

Newer filesystems enforce that a filename is a valid Unicode string, but on many widely used filesystems,
filename can be an arbitrary string of bytes.\footnote{Unless the byte represents a forbidden character --
forward-slash or null byte on UNIX, shell expansion characters on Windows and similar.} In effect,
a filesystem can store names that cannot be properly decoded in the system's character set. This presents
a problem for the transcoding process.  An obvious solution is to ignore invalid filenames, but that is
arguably not very useful. It is seemingly better to use replacement characters for the undecodable sequences,
but in practice, this does not help very much, as this way of encoding is not roundtrip-safe. The client can
get a listing that contains invalid filenames, but cannot specify them, because files with the modified names
do not actually exist on the server. Moreover, it is impossible for the client to tell apart invalid filenames
from valid ones that happen to contain the replacement characters, unless the replacement characters are
forbidden in valid names.

Explicit support for undecodable filenames is possible in several ways, e.g., through lossless transcoding of
undecodable characters, or by refering to files in some other way than by name.


\subsection{Download}

Downloading, i.e., transfering files from server to client, is arguably the most basic capability provided by
a file transfer protocol. Client will request a file and server will respond by sending a binary stream of the
file's contents. Failure modes are obvious: the identifier does not point to a file; file exists but cannot be
read; or this client does not have permission to access the file.

In basic form of download, the server will send full contents of the file as a continuous stream, possibly
preceded by length of the stream. However, it is often more useful to specify a range for reading. This allows
resuming of interrupted downloads, seeking in media streams and the like. It is also very useful if the client
wants to implement filesystem-like behavior over the protocol.

A general-purpose protocol will treat files as opaque streams. However, in a specialized protocol, it might be
useful to understand structure of the file. This allows the client to perform more abstract operations --
e.g., query contents of the file as a database, download only thumbnail or embedded metadata etc.

\subsection{Upload}

Transfering from client to server is more complicated. In addition to checking for permissions, the server
must ensure that there is enough disk space, resolve issues with overwriting existing files, as well as
gracefully handle concurrent modifications. There might also be issues with missing path components -- trying
to create file in a directory that does not exist. This results in several new failure modes that must be
expressed in the protocol. However, at this level, it is sufficient to add new error codes, or specify that
a single error code covers many different types of failure.

\subsection{Directories}

Although it might be tempting to handle directories as a special case of files, directories are very
different. Where files are opaque blobs, directories are inherently structured, so the protocol must represent
an API for manipulating the structure. A general-purpose protocol should provide at least the following
operations:

\begin{itemize}
	\item list contents of directory
	\item create file/directory
	\item delete file/directory
	\item rename a directory entry
\end{itemize}

Listing is pretty straightforward. The only potential issue is that it is difficult to know its length in
advance. This makes certain kinds of message framing more challenging.

Files can be created implicitly when attempting to write to them. Creating an empty file can be accomplished
by a zero-length write. On the other hand, there is no equivalent of ``zero-length write'' for implicit
creation of directories. Therefore, creating a directory needs to be a separate operation.

Deleting files and directories is similar in that it means removing one directory entry. However, if
implemented in a same operation, it has to account for the possibility that the directory being removed is not
empty. Then it can either offer the option to recurse into non-empty directories, or return a special error
condition to indicate this.

\subsection{Renaming and moving}

Within a single filesystem, renaming and moving is usually performed by the same operation, which is
essentialy ``change this complete path to a different one''. Since a move operation only consists of relinking
a directory entry, it functions the same on individual files as it does on directories, even non-empty
ones.\footnote{Unlike the remove operation, rename/move does not affect contents of the directory.} A protocol
can export this functionality as a single command.

Things get more difficult when the new pathname would end up on a different filesystem. To move a file from
one filesystem to another, you first need to make a copy of the file, then remove the original.  This is hard
to do atomically, even in case of a single file. When attempting to move a non-empty directory, the
copy-and-remove operation has to be done recursively on its contents.

In effect, rename/move operations either need to be (potentially) non-atomic, or fail on operations crossing
filesystem boundaries. In the latter case, the client would be expected to perform the move ``by hand'', by
copying to target and removing sources. But to do this efficiently, the protocol would have to expose an
on-server copy operation, otherwise the client would need to download and reupload all of the data.

\subsection{Server filesystem structure}

There are two reasons for the client to be interested in the server's filesystem configuration: free space
information, and knowledge about filesystem boundaries. Exposing this information is particularly complicated
on UNIX-like systems, where it is common to use arbitrary directories as mount points for different
filesystems. One possible solution is having a ``statfs'' operation that returns filesystem information for
particular file or directory. It would then be the client's responsibility to use this operation whenever
necessary. Another possibility is to make filesystem information part of directory listing.

\subsection{Metadata}

Apart from contents of files and directories, the filesystem stores metadata about entries: file size,
creation date, last modification date, filesystem attributes, ownership and access rights, link count etc.
Although it is not essential, a full-featured protocol will expose operations to query and modify this kind of
metadata. It is also very useful to send this information along with directory listing -- querying each entry
in a directory with hundreds or thousands of files would be woefully inefficient. On the other hand, sending
the full set of metadata would make the listing significantly larger. To avoid that, clients might specify
what kind of metadata to include in the listing.

Filesystem metadata is obviously different on different filesystems and different operating systems. The
protocol can provide a common subset, a union set with missing fields, or platform-specific functions.

Some filesystems support custom metadata fields, or access control lists (ACLs) for fine-grained rights
management. These are rarely used in end-user software, but if a protocol is supposed to provide
filesystem-like functionality, it should also expose this kind of metadata.

\subsection{Locking}

Modern operating systems provide mechanisms for one process to lock a file, preventing other processes from
accessing or modifying it concurrently. It is straightforward to expose these operations through a network
protocol, but doing so without careful consideration will lead to problems. Locking is usually designed and
used as a guarantee of atomicity and/or consistency, and it is difficult to provide such guarantees when
working over an unreliable network. Transmission errors and lost connections may result in an inconsistent
state of the locked file if the lock is released too soon, or in deadlocks if the lock is held for too long.

%%%

\section{Other considerations}

When designing a protocol, it is not enough to look at a checklist of desired capabilities. We must also
consider how the protocol behaves in practice, and how it functions as a part of a larger system. In this
section, we examine various properties of the protocol and its behaviors in a more goal-oriented manner.

\subsection{Authentication}

Apart from file serving capabilities, a protocol should have means to identify clients and verify that they
are who they claim to be in order to properly enforce access rights. Most commonly, clients have to identify
themselves by providing user name and a valid password for that user. Other possibilities include using
challenge-response schemes, public key authentication, or a trusted third party. Some protocols include
authentication mechanisms directly, other rely on the transport layer to provide an authenticated session.

From the other side, it is often useful to enable public unauthenticated access. A good way to accomplish this
is to make authentication optional. This might be difficult to do when authentication comes from the transport
layer, which might or might not support unauthenticated channels. Another way is to establish a convention
that a certain username (such as ``anonymous'' or ``guest'') does not require credentials, but depending on
the specifics of the authentication scheme, this might be hard too.

\subsection{Encryption}

Public networks, and especially the Internet, make no guarantees of confidentiality. Even if you are not
broadcasting on an open Wi-Fi network, your communication is routed through a number of untrusted third
parties. It would be unwise to transmit anything without encryption, especially so in a file transfer protocol
that might potentially be used to work with confidential data.

Protocols often rely on the transport layer to provide encryption. Probably the most popular solution today is
the Transport Layer Security (TLS) protocol~\cite{rfc5246}, which works on top of TCP. Its popularity also
means that it receives a lot of attention from the cryptography community, so its security properties are
relatively well understood.\footnote{This mostly means that the TLS scheme has ``withstood the test of time'':
under such detailed scrutiny, the likelihood of a major problem remaining undiscovered is lower than in less
studied schemes. In theory, anyway.} TLS is based on public-key cryptography, and provides cryptographic
confidentiality and authenticity. That means that an attacker cannot decrypt your messages or forge illicit
ones without knowing the corresponding private key.

This kind of authenticity guarantee is necessarily limited. You can prove that you are communicating with the
rightful owner of a key, bud you still need to connect the key to an identity. In a so-called {\it Man in the
Middle} (MitM) class of attacks, the attacker sets up an intercepting proxy with their own key. When the
client connects, an encrypted channel is established using the attacker's key instead of the legitimate one.
The proxy can then forward client requests to the original server, so that nothing looks wrong from the
client's perspective, or actively modify the communication.

To prevent MitM attacks, the client must be able to verify ownership of the server key. This, in general, is
a hard problem. Either the key must be known in advance, or the client needs to trust a third party that can
vouch for the server's identity. The latter scheme is commonly called a public-key infrastructure (PKI), and
it is the solution used by TLS. Every server presents a public-key certificate that links their key to
indentity information. The certificate may also carry a cryptographic signature from another party, whose
certificate can in turn be signed by another, forming a chain of trust. Operating systems and popular
applications such as browsers contain a core set of trusted {\it root certificates}, and if the chain of trust
can be traced back to a root certificate, the identity information of the server is considered authentic.

Knowing the keys of every potential server of interest is obviously impossible, and due to real-world
constraints, it is often impractical for server operators to obtain trusted signatures. To achieve at least
some level of MitM protection in these conditions, clients should store the server's public key on first
connection and check that it doesn't change on subsequent attempts.

\subsection{Resistance to attacks}

Security of server and clients mainly hangs on their implementations, not on the protocol itself. However,
protocol designers should strive to minimize the attack surface by minimizing the number of features that
might interact with each other in unpredictable ways. Generally, the simpler the protocol definition, the
easier it is to implement, and the less chance for introducing security (or other) bugs in the implementation.

Another consideration is vulnerability to Denial of Service attacks. Individual operations should be designed
so that a simple command from the client cannot use up more than a ``fair share'' of computational capacity of
the server.\footnote{For instance, it is possible to construct very short XML documents that require
inordinate amounts of memory and processing time to fully parse, due to recursive explosion of entity
expansion. Compression algorithms face a similar problem.} Otherwise, malicious clients could abuse such
functionality to tie up all of server's resources. Of course, in the end, it is up to the server
implementation to set up and enforce resource limits, but protocol design should not actively make this task
harder.

\subsection{Performance}

There are several factors that come under this heading. One of them is protocol overhead: how much data is
sent over the wire when transfering a file, compared to size of the file itself.  This includes per-file and
potentially per-session setup cost, overhead of session management, size of protocol messages, framing
overhead and so on. Obviously, a protocol that requires a ten-byte packet for every byte of payload would not
fare too well in this regard. Lowest achievable overhead would be in a protocol consisting of a short setup
sequence followed by unbroken transmission of the file contents.

This would, however, vastly increase response times. Users want to be able to run file transfers in the
background while continuing to browse the server. Even more demanding is usage as a filesystem, where random
access is essential. Pausing all I/O operations until file transfer is finished would be unacceptable, and
opening a new connection for every parallel operation is a poor solution. It should be possible to interleave
several operations within a single session. To achieve that, it is necessary to limit the size of each
request-response exchange.

Some operations take longer to execute than others. Even if we count reading from a local hard drive as
near-instanteous, the protocol might support operations like server-side copy, which can take a long time to
complete. If every client request must be followed by a corresponding response, waiting for one long-running
operation would kill latency. The protocol could allow the server to reorder responses, so that it can service
upcoming requests before the long-running operation completes. Alternately, there could be a mechanism for the
client to launch a long-running operation and monitor its progress without waiting for completion.

\subsection{Server state}

The terms ``stateless'' and ``stateful'' in relation to protocols are not defined precisely. But loosely
speaking, in a stateless protocol, the server does not retain any request- or client-related information
between client requests.  Each request could come in stand-alone, in its own session, and the result would be
the same as if several requests came one after another within a single session.

There are several advantages to statelessness. Most obviously, a stateless protocol copes better with network
failures. Loss of connection interrupts at most one request, which can be immediately repeated when
connectivity is restored. In the same vein, server can restart between requests and if this happens fast
enough, clients might not even notice the outage. This property also lends itself well to load balancing:
a number of separate computers can be deployed running the server software, and clients can be randomly
directed to any of them without the need to synchronize state between the servers.

Statelessness is limiting in that every request must be self-contained. That means that there are no
authenticated sessions, every request must be authenticated separately. Same goes for other properties, such
as filenames: in a sequence of requests for reading from the same file, the filename would have to be repeated
in each of them.  Advantages of stateless protocols don't apply well to long-running operations, because the
progress of the operation itself is state and it is lost in case of failure.

\subsection{Idempotence}

Idempotence is the property of operations where subsequent applications of the same operation do not change
the initial result. In the context of a file transfer protocol, this means that repeating a particular command
with particular arguments any number of times will cause at most one set of changes to the server filesystem.
Ideally, each invocation will also return the same result.

This is very useful for recovering from network failures. If the client issues a request and does not receive
a response, it can't be sure whether the request actually arrived on the server and the response was lost, or
whether the request itself was not delivered. Conversely from the other side, if the server sees a repeated
request, it might not be able to know if the client is retrying a request that appeared as failed, or if it
intends to repeat the same request for the second time. Idempotent operations ensure that this does not
matter: the server can perform the operation again safely, without doing something the client did not intend.

Read operations are idempotent by nature. Unless another user or process modifies the filesystem, reading the
same position from the same file will always return the same result.  This holds for listing directories,
querying file attributes etc. Setting metadata values or writing same data into the same file offset
repeatedly is also idempotent. However, an {\it append} operation is not, because it writes to current end of
file, which might have been changed by the previous {\it append}.

Rename and delete operations could be considered idempotent in the sense that repeating the operation will not
cause further changes. However, the first successful application will remove the source file, and subsequent
attempts might result in an error. This should not matter to the client: for deletion, the desired result has
apparently been accomplished; and for rename, it is easy to query the supposed new path in order to
distinguish between external modification and previous operation succeeding.

The operations mentioned so far lose their idempotence properties when interacting with other clients. Even
a repeated read can give different results if the file is externally modified at the same time, and can even
return an error when another client removes the file. This is unavoidable without proper locking, but replay
attempts exacerbate the problem by enlarging vulnerable windows and introducing new faulty interaction modes,
such as repeatedly deleting a file placed by a different client.

This is especially important to note with file and directory creation operations. When running alone, the
situation is very similar to the rename and delete discussion: an operation will repeatedly fail after first
successful attempt. However, in the absence of a dedicated locking mechanism, many programs use temporary
files and especially directories to indicate that a lock on particular resource is held.  For this use case,
it is very important to distinguish whether the create operation succeeded (the caller now holds the lock) or
failed (somebody else holds the lock).

To prevent faulty replays, the server needs to retain results of recent operations, distinguish replay
attempts from actual repeated requests, and return the original result where appropriate. This must be done
with care, so that malicious actors cannot trigger the replay detection for other clients. A possible way to
accomplish this is by sending a unique token with each request. The server can then store the token along with
a result, and return that when it encounters the same token again. If the token is stored as a metadata entry
for the affected filesystem object, it can even provide replay protection for fully stateless servers.


\subsection{Client-side caching}

Applications that directly implement a network protocol tend to ``know how to use it'' -- how to perform their
actions and service user requests in a manner that is efficient over the given network protocol. For instance,
a file manager with network support can use operations like server-side copy; a media player knows which parts
of the file are required for complete playback and which can be discarded after a seek; database editor can
efficiently predict which blocks to read ahead depending on what operation the user is performing.
Furthermore, in day-to-day operation, users can work with stale data that is minutes or even hours old -- and
where required, the application can provide a ``refresh'' function to invalidate caches manually.

This is not the case when the protocol is used to emulate a local filesystem. In such situation, the
implementing application only receives requests that correspond to filesystem primitives, and it needs to
provide low latency for applications not designed with networking in mind. Here, local caching and timely
cache invalidation is especially important.

Normally, a file transfer protocol would not make special arrangements for caching. It might be beneficial to
at least informally describe the sort of behavior that can be expected. The simplest caching policy is
client-dependent, i.e., the onus is on the client to periodically check freshness of its cache. As for write
cache, it is traditionally flushed when an user application closes the file.

Specialized protocols, or protocols designed to act as filesystems, may prescribe more specific caching
policies or even facilitate mechanisms for cache invalidation triggered by the server.  As an example of one
of the simpler approaches, the client can request to be notified of all changes on a given set of files.
A more complicated example is the server giving virtual ownership of a file to a client, and notifying it when
other clients want to access the file so that the virtual owner can sync back its changes.

Note that it is significantly easier for the server software to maintain client caches and concurrent access
invariants if it is the only process that works with the published filesystem.
